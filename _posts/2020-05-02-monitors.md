---
layout: post
title: "Time to upgrade your monitor"
category: blog
summary: "A non-comprehensive and opinionated guide to best monitor for programming"
draft: true
---

<style>
    @media (min-width: 736px) {
      .cover { width: 736px; height: 472px; max-width: unset; margin: 0 -96px; }
    }
    .fig_smoothing { width: 600px; height: 220px; background-image: url(smoothing_anim.png); background-size: 100%; margin-bottom: 1em; border-radius: 3px; }
    .fig_smoothing:hover { background-position: 0 -220px; }
</style>

<img class="cover" src="cover.png">

I am a programmer. I do not deal with digital painting, photo processing, video editing. I don’t really care for wide gamut or even proper color reproduction. I spend most of my days in text browser, text editor and text terminal, looking at barely moving letters.

So I optimize my setup to showing really, really good letters. A good monitor is essential for that. Not nice to have. A MUST. And in “good” I mean, as good as you can get. These are my thoughts, based on my own experience, on what monitors work best for programming.

# Low-density displays

Anything below 150 ppi — Don’t. Just don’t. You CAN NOT get good letters with those. It’s not a matter of switching from Mac to Windows (or from Windows to Mac), or fine-tuning Clear type, or choosing right fonts, or configuring gamma. No. It is plain and simple impossible. No amount of tweaking and trickery can make text look good on low-res display. Period. The sooner we all, collectively, accept that unmovable fact, the faster we move into the future.

## Too few pixels

First, there are simply not enough pixels to draw a letter. Let’s take Consolas, a font that was developed specifically for programmers. Microsoft worked really hard to fine-tune it for low-resolution rendering. We set it at 14px, which is default in VS Code (and people often go lower than that!):

<figure>
    <img src="consolas_gray_1x.png" style="height: 180px">
    Consolas at 14px, macOS
</figure>

On that size, capital letter B takes up mere 6×9 pixels on screen. Lowercase letters only have 7 (seven!) pixels to work with. That’s NOT MUCH. I have more fingers than that. No matter how good font is designed, it’s hard to show anything when all you have is seven pixels. Anything slightly more complex than “T” or “H” becomes an inlegible pixel mess.

Look at the “g” in the picture above. It’s hard to say where strokes start or end, or even how many there are to begin with. It’s just a random gray noise, or a checkerboard, but not a letter. This is a letter:

<figure>
    <img src="consolas_24x.png" style="height: 180px">
    Consolas at 168px
</figure>

It’s a shame, really, watching these beatiful, fine details clapmed into mere 70 pixels.

## The dreadful hinting

To fight this problem, Windows uses pretty aggressive hinting. Basically, it just bends and moves letter strokes to the nearest pixel, ensuring more crisp boundaries.

And it works! Fonts do look better with hinting than without it:

<figure>
    <img src="hinting.png" style="height: 180px">
    No hinting (macOS) → Hinting (Windows)
</figure>

Don’t get your hopes up thought: it’s still a mess. It doesn’t make text look _good_. It makes it look _better_, but it is still bad.

The main problem with hinting, though, is that it destroys letter shapes. Pixels are rendered not where they are supposed to be, but rather where pixel grid happens. To give you a taste:

<figure>
    <img src="ttf_hinting.png" style="height: 243px">
    Verdana (k) and Times New Roman Italic (z) at 13px. <a href="http://web.archive.org/web/20160304021305/http://antigrain.com/research/font_rasterization/index.html">Source</a>
</figure>

The idea is that it’ll look better when rendered to the actual pixels.

But even if we just look at vertical hinting of horizontal stems, it’s still changes font way too much:

<figure>
    <img src="hinting_align.png" style="height: 180px">
</figure>

See how horizontal stems are offset from their actual position in the vector font file? The error here is as big as ¼ pixel!

But hey! If you don’t know what Consolas is *supposed* to look, who cares? Well, sometimes problems are more obvious: circles are not circles, equal distances become not equal, proportions are all wrong, what supposed to be small becomes huge and vice versa, etc. Here:

<figure>
    <img src="hinting_numbersign.png" style="height: 180px">
</figure>

After moving horizontal stems to match pixel grid (by offsetting them up to ½ pixel!), Windows is having a hard time splitting 7 other pixels into three equal gaps. Unfortunately, alternative is no better:

<figure>
    <img src="no_hinting_numbersign.png" style="height: 180px">
</figure>

This is the game that cannot be won. Text on low-DPI display can’t look good because *there are simply not enough pixels*. It’s that simple, really.

## A fractional pixel

Then there’s ClearType (MicrosoftType), FreeType (LinuxType), CoolType (AdobeType), MacOSXType and other forms of subpixel rendering. The idea is simple, really. Your display’s pixel consist of three vertical subpixels, each responsible for their own color. We can light them up individually, effectively tripling horizontal resolution! 

We hope to get this:

<figure>
    <img src="subpixel_bw.png" style="height: 180px">
</figure>

But because subpixels are colored, we get something that looks like this:

<figure>
    <img src="subpixel_rgb.png" style="height: 180px">
</figure>

On a real screen fully-lit RGBs will blend into flat white color. What you actually percieve is much closer to this:

<figure>
    <img src="subpixel_color.png" style="height: 180px">
</figure>

It is better in terms of horizontal resolution, but at the same time black-on-white text gets slight teal-and-orange halo around it. It’s not *that* bad, not even annoying, but you _can_ see it.

What I am trying to say is: all these tricks work. Having them is strictly better than not having them. For low-DPI displays all those are *a must*. But at the same time they are a tight compromise made in times where we didn’t have better displays. But we have now. It’s time for all those tricks to go.

<figure>
    <img src="retina.png" style="height: 180px">
    Consolas 14px with ClearType and hinting → Consolas 14px @2x
</figure>

# Retina Macbooks

If you are using Macbooks with retina screens, there are two things you absolutely must do.

## Font smoothing should go

First, turn OFF “Font smoothing” in System Preferences → General:

<figure>
    <img src="no_smoothing.png" style="height: 220px">
</figure>

I’m not sure what the default value for it is these days, but make sure it’s OFF anyways.

That preference name is misleading. It used to be called “LCD font smoothing”, which suggested subpixel antialiasing. But it doesn’t control it, at least, not since 10.14 Mojave.

The other thing the name might suggest is that without it your fonts will not be smoothed at all. This is not the case either.

What it actually does is it makes font slightly bolder:

<figure>
    <div class="fig_smoothing"></div>
    Hover over the image to see the difference
</figure>

So, why would you want it off? The thing is, there’s no magical way to make arbitrary font bolder. I’m not sure what exactly macOS does. My guess is, it is either blurring or drawing non-zero outline around letters:

<figure>
    <img src="smoothing_artefacts.png" style="height: 156px">
</figure>

So imagine a font designer who carefully balanced every letter, placed each point down to 1/100 of the pixel, only to be ignored by dumb software that thinks it knows better.

What does that mean for us, programmers? If you take a font that was hand-optimized for particular pixel size (which many programming fonts are, e.g. [Input at 11px](https://input.fontbureau.com/info/) or [Monoid at 12px](https://larsenwork.com/monoid/)), it will be rendered blurry despite all the efforts. 

<figure>
    <img src="smoothing_input.png" style="height: 372px">
</figure>

And all the other fonts, including system ones, will be slightly more blurry than they need to be.

## Integer scaling

When I bought my first (and world’s first) Retina Macbook Pro in 2012, it was exactly what was advertised: 2× scaling, every logical pixel rendered to 2×2 screen ones. 2880×1800 screen would be rendered from 1440×900 logical source.

Sadly, reason has left Apple since and at some point Macbooks started to get weird non-integer scaling as default. E.g. 2880×1800 screen would have 1680×1050 logical resolution. That is 1.7142857143... scaling factor, or 12/7.

Why? I guess, someone figured more space is better. The problem is, it doesn’t give you that much space, compared to 2× just mere 15%. But that comes at the cost of losing any chance to render ANY pixel-crisp image at all!

Let’s see. 12/7 scaling factor means that for every 7 logical pixels there are 12 corresponding screen pixels. Meaning, every 7 pixels you have a chance do draw a 7-pixel tall rectangle and that’s your only chance to align with the pixel grid.

<figure>
    <img src="12by7_grid.png" style="height: 336px">
</figure>

Move 1 pixel up or down—you lose. Make it 1px taller or shorter—you lose.

<figure>
    <img src="12by7_grid_offset.png" style="height: 336px">
</figure>

A pixel-perfect line? Too bad you can’t specify 7/12 of a pixel as the line width. Even worse, each line looks differently depending on its vertical position:

<figure>
    <img src="12by7_lines.png" style="height: 204px">
</figure>

It shouldn’t come as a surprise that modern icons are mostly made of single-pixel-wide strokes:

<figure>
    <img src="12by7_ui.png" style="height: 274px">
    Top: 2× scale, bottom: same after 12/7 downscaling
</figure>

It’s hard to imagine someone who want to look at *that* on purpose.

What happens with the text? Nothing good. First it is rendered pixel-crisp at 2× resolution, then downscaled at 85.7142857143...% to fit into physical pixels:

<figure>
    <img src="12by7_text.png" style="height: 243px">
    Monoid at 12px. Top: 2× scale, bottom: same after 12/7 downscaling
</figure>

That’s right, UI isn’t even rendered in that weird target resolution. Every Mac app thinks it is rendering at 2×, and only after that OS scales it down to the target resolution. There’s a lot of precision and nuance lost due to this two-step process.

In my opinion, nothing can do more harm to the look of UI than this. Even old low-dpi UIs are better, since their lines at least align with pixels.

And don’t forget: this is the default. Every Macbook is shipped with these settings. Millions of people are working not knowing they were robbed of the joy of the retina screen.

Luckily for us, this is easy to fix (at least for now). Go to System Preferences → Displays, uncheck Default and select 2× resolution instead:

<figure><img src="scaling.png"></figure>

This will make everything on screen slightly bigger, leaving you (slightly!) less screen estate. This is expected. My opinion is, notebook is a constrained environment by definition. Extra 15% won’t magically turn it into huge comfortable desktop. But at least you can enjoy that gorgeous screen. Otherwise, why would you buy retina screen at all?


https://www.acer.com/ac/en/US/content/model/UM.HX3AA.P02
https://www.asus.com/us/Monitors/ROG-SWIFT-PG27UQ/

https://steipete.com/posts/the-lg-ultrafine5k-kerneltask-and-me/
https://bjango.com/articles/macexternaldisplays/